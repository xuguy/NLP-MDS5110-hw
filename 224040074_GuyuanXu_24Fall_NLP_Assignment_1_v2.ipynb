{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yFFKuDZB_ow"
      },
      "source": [
        "\n",
        "# Assignment 1: Exploring Word Embeddings\n",
        "**Course Name:** Natural Language Processing (CSC6052/5051/4100/DDA6307/MDS5110)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_qFy6J9EgJe"
      },
      "source": [
        "*Please enter your personal information (make sure you have copied this colab)*\n",
        "\n",
        "**Name:**\n",
        "\n",
        "**Student ID:**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbC_Md4Gicaz",
        "outputId": "8161f3ca-1482-4e85-edfc-a9eff0370abf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "student id: 224040074\n"
          ]
        }
      ],
      "source": [
        "# test\n",
        "print(f'student id: 224040074')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQDMk5Uu1niv"
      },
      "source": [
        "## Assignment Requirements\n",
        "\n",
        "This Colab file includes all contents for Assignment 1.\n",
        "\n",
        "#### You are required to:\n",
        "\n",
        "1. **Make a copy of the provided Google Colab file.**  \n",
        "   First, you need to make a copy of the provided file into your own Google Drive. To accomplish this, open the Colab file link, navigate to `File` → `Save a copy in Drive`.\n",
        "\n",
        "2. **Execute the notebook to generate results.**  \n",
        "   You can click on \"Connect to GPU\" to apply for a free T4 GPU. Then, you can press the large play button to run a code cell.\n",
        "\n",
        "3. **Complete the Necessary Parts.**  \n",
        "   Some sections of the code are incomplete and require your input, especially pay attention to the parts marked with **<font color=\"red\">[Task]</font>**. These sections are critical for scoring the assignment.\n",
        "\n",
        "For more detailed instructions, refer to [Working with Google Colab](https://docs.google.com/document/d/1vMe8kC-oSyP3w7rIurDbG3NqfyQw7sZJ2C_S2ngtQnk/edit?usp=sharing).\n",
        "\n",
        "## Submission Guidelines\n",
        "\n",
        "Follow these steps to submit your assignment:\n",
        "\n",
        "1. **Export the Notebook:** Navigate to `File` → `Download .ipynb` to download your notebook.\n",
        "\n",
        "2. **Upload Your File:** Access the [Blackboard system](https://bb.cuhk.edu.cn/) and upload your `.ipynb` file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0gO12GJE06O"
      },
      "source": [
        "\n",
        "## Overview\n",
        "\n",
        "*Assignment 1* consists of two tasks:\n",
        "- Task 1: Train Word Embeddings with Word2Vec (5 points)\n",
        "- Task 2: Explore word embeddings (3 ponits)\n",
        "- Task 3: Utilize word embeddings (2 points)\n",
        "\n",
        "Your task is to **run all the code in this script** and complete the parts marked with <font color=\"red\">[task]</font>.\n",
        "\n",
        "## Prerequisite\n",
        "If you're new to Python, Numpy, or PyTorch, consider these tutorials for a quick start:\n",
        "- [Python-Numpy-Tutorial](https://cs231n.github.io/python-numpy-tutorial/)\n",
        "- [Introduction to PyTorch](https://colab.research.google.com/drive/1obAmmGHsMizB38aiZJ_-L1bVMT5KOLMd?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwAJjpp-K8RQ"
      },
      "source": [
        "## Task 1: Train Word Embeddings with Word2Vec\n",
        "\n",
        "**In this task, you will implement and train your own Word2Vec model.**\n",
        "\n",
        "Before diving in, let's clarify what Word2Vec is.\n",
        "\n",
        "Its core concept is straightforward: you can infer the meaning of a word from its neighbors - the words that frequently appear in the same context. Consider this illustration:\n",
        "![Contexts](https://image.ibb.co/mnQ2uz/2018_09_17_21_07_08.png)\n",
        "\n",
        "A basic approach is to use the context word counts as meaningful word vectors. Take this simple corpus for example:\n",
        "\n",
        "```\n",
        "The red fox jumped\n",
        "The brown fox jumped\n",
        "```\n",
        "\n",
        "The count vectors would look like this:\n",
        "```\n",
        "        the fox jumped red brown\n",
        "red   = (1   1    1     0    0)\n",
        "brown = (1   1    1     0    0)\n",
        "```\n",
        "\n",
        "Notice how `red` and `brown` have similar vectors! We're close to solving the problem, but the goal is to obtain more compact embedding vectors.\n",
        "\n",
        "This is where Word2Vec algorithms come into play. They construct embedding vectors based on the word's neighbors in the corpus.\n",
        "\n",
        "For a more detailed introduction, check out this post: [king - man + woman = queen; but why?](http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html).\n",
        "\n",
        "Let's do some preparation work before moving to the interesting stuff.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buz0YOV5WrDL",
        "outputId": "c9e710a4-16eb-4309-e32a-894df98f66ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.12.3\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rK5IxppLRHM"
      },
      "source": [
        "### **1.1 Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blHR2OjyO5mg"
      },
      "source": [
        "Environment installation and data download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vacV4BIFI8l",
        "outputId": "97bb9e6d-2019-4d7f-9890-c67a6e3637e0"
      },
      "outputs": [],
      "source": [
        "# i disabled some packages because no need of them\n",
        "\n",
        "# !pip3 -qq install torch==1.1\n",
        "# !pip -qq install nltk==3.8 #3.2.5\n",
        "\n",
        "!pip -qq install nltk==3.8\n",
        "!pip -qq install gensim\n",
        "!pip -qq install bokeh==3.2.0\n",
        "\n",
        "# !pip install nltk\n",
        "# !pip install gensim\n",
        "# !pip install bokeh\n",
        "\n",
        "# -qq means: do not print message while executing command\n",
        "!wget -O quora.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1ERtxpdWOgGQ3HOigqAMHTJjmOE_tWvoF\"\n",
        "!unzip -o quora.zip\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from IPython.display import clear_output\n",
        "%matplotlib inline\n",
        "np.random.seed(42)\n",
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tqdm import tqdm\n",
        "\n",
        "# please ignore ERROR of package version conflict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgSYdy30O_UZ"
      },
      "source": [
        "1. Tokenize and lower-case texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t27TcJKJO-ry",
        "outputId": "590d1f54-5fd3-4c4c-b600-1d85e20d0e84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50000/50000 [00:02<00:00, 24295.42it/s]\n"
          ]
        }
      ],
      "source": [
        "quora_data = pd.read_csv('train.csv')\n",
        "\n",
        "quora_data.question1 = quora_data.question1.replace(np.nan, '', regex=True)\n",
        "quora_data.question2 = quora_data.question2.replace(np.nan, '', regex=True)\n",
        "\n",
        "texts = list(pd.concat([quora_data.question1, quora_data.question2]).unique())\n",
        "texts = texts[:50000] # Accelerated operation\n",
        "print(len(texts))\n",
        "\n",
        "tokenized_texts = [word_tokenize(text.lower()) for text in tqdm(texts)]\n",
        "\n",
        "assert len(tokenized_texts) == len(texts)\n",
        "assert isinstance(tokenized_texts[0], list)\n",
        "assert isinstance(tokenized_texts[0][0], str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9K7y4mD2UK2",
        "outputId": "1be68bcf-823c-4a16-f770-2a91e28b5859"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['what',\n",
              " 'is',\n",
              " 'the',\n",
              " 'step',\n",
              " 'by',\n",
              " 'step',\n",
              " 'guide',\n",
              " 'to',\n",
              " 'invest',\n",
              " 'in',\n",
              " 'share',\n",
              " 'market',\n",
              " 'in',\n",
              " 'india',\n",
              " '?']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_texts[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYoj91iDDDfT"
      },
      "source": [
        "2. Collect the indices of the words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PL471pGjuVN",
        "outputId": "af461a4e-c258-4e1c-f2d6-f76e8f241ddb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 7226\n",
            "Tokens count: 623563\n",
            "Unknown tokens appeared: 35607\n",
            "Most freq words: ['?', 'the', 'what', 'is', 'how', 'i', 'a', 'to', 'in', 'do', 'of', 'are', 'and', 'can', 'for', ',', 'you', 'why', 'it', 'best']\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "MIN_COUNT = 5\n",
        "\n",
        "words_counter = Counter(token for tokens in tokenized_texts for token in tokens)\n",
        "word2index = {\n",
        "    '<unk>': 0\n",
        "}\n",
        "\n",
        "for word, count in words_counter.most_common():\n",
        "    if count < MIN_COUNT:\n",
        "        break\n",
        "\n",
        "    word2index[word] = len(word2index)\n",
        "\n",
        "index2word = [word for word, _ in sorted(word2index.items(), key=lambda x: x[1])]\n",
        "\n",
        "print('Vocabulary size:', len(word2index))\n",
        "print('Tokens count:', sum(len(tokens) for tokens in tokenized_texts))\n",
        "print('Unknown tokens appeared:', sum(1 for tokens in tokenized_texts for token in tokens if token not in word2index))\n",
        "print('Most freq words:', index2word[1:21])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<unk>', '?', 'the', 'what', 'is']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 0-5 represent words ['<unk>', '?', 'the', 'what', 'is']\n",
        "index2word[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHEQOelqQwqP"
      },
      "source": [
        "3. collect the context words\n",
        "\n",
        "First of all, we need to collect all the contexts from our corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ocrsXgaynYPG"
      },
      "outputs": [],
      "source": [
        "# i think this is the most important part of training a word embedding network\n",
        "# skipgram and cbow differs mostly in training/label data structure.\n",
        "\n",
        "def build_contexts(tokenized_texts, window_size):\n",
        "    contexts = []\n",
        "    for tokens in tokenized_texts:\n",
        "        for i in range(len(tokens)):\n",
        "            central_word = tokens[i]\n",
        "            context = [tokens[i + delta] for delta in range(-window_size, window_size + 1)\n",
        "                       if delta != 0 and i + delta >= 0 and i + delta < len(tokens)]\n",
        "\n",
        "            contexts.append((central_word, context))\n",
        "\n",
        "    return contexts\n",
        "\n",
        "contexts = build_contexts(tokenized_texts, window_size=2)\n",
        "# save a copy of contexts for later visualization purpose\n",
        "contexts_copy = contexts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('what', ['is', 'the'])\n",
            "('is', ['what', 'the', 'step'])\n",
            "('the', ['what', 'is', 'step', 'by'])\n",
            "('step', ['is', 'the', 'by', 'step'])\n",
            "('by', ['the', 'step', 'step', 'guide'])\n",
            "('step', ['step', 'by', 'guide', 'to'])\n",
            "('guide', ['by', 'step', 'to', 'invest'])\n",
            "('to', ['step', 'guide', 'invest', 'in'])\n",
            "('invest', ['guide', 'to', 'in', 'share'])\n",
            "('in', ['to', 'invest', 'share', 'market'])\n"
          ]
        }
      ],
      "source": [
        "# check contexts data:\n",
        "for i in contexts[:10]:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o_ePiZ7wfpT"
      },
      "source": [
        "Check, what you got:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyQNK-9SBdb9",
        "outputId": "3d46cf7f-5d62-4636-c139-a91c7531b893"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('what', ['is', 'the']),\n",
              " ('is', ['what', 'the', 'step']),\n",
              " ('the', ['what', 'is', 'step', 'by']),\n",
              " ('step', ['is', 'the', 'by', 'step']),\n",
              " ('by', ['the', 'step', 'step', 'guide'])]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contexts[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbQKln_6yC4l"
      },
      "source": [
        "4. Convert to indices\n",
        "\n",
        "Let's convert words to indices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hOPRlKlLvUBA"
      },
      "outputs": [],
      "source": [
        "# dict.get(keyname, value): return value if keyname not found in dict\n",
        "# here, unknown words are all assign to indice 0\n",
        "contexts = [(word2index.get(central_word, 0), [word2index.get(word, 0) for word in context])\n",
        "            for central_word, context in contexts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('what', ['is', 'the']) - (3, [4, 2])\n",
            "('is', ['what', 'the', 'step']) - (4, [3, 2, 1310])\n",
            "('the', ['what', 'is', 'step', 'by']) - (2, [3, 4, 1310, 65])\n",
            "('step', ['is', 'the', 'by', 'step']) - (1310, [4, 2, 65, 1310])\n",
            "('by', ['the', 'step', 'step', 'guide']) - (65, [2, 1310, 1310, 2105])\n",
            "('step', ['step', 'by', 'guide', 'to']) - (1310, [1310, 65, 2105, 8])\n",
            "('guide', ['by', 'step', 'to', 'invest']) - (2105, [65, 1310, 8, 629])\n",
            "('to', ['step', 'guide', 'invest', 'in']) - (8, [1310, 2105, 629, 9])\n",
            "('invest', ['guide', 'to', 'in', 'share']) - (629, [2105, 8, 9, 696])\n",
            "('in', ['to', 'invest', 'share', 'market']) - (9, [8, 629, 696, 399])\n"
          ]
        }
      ],
      "source": [
        "# check word-to-indices data structures\n",
        "for context, indices  in zip(contexts_copy[:10], contexts[:10]):\n",
        "    print(f'{context} - {indices}')\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGfhLR6x8D3r"
      },
      "source": [
        "### **1.2 Continuous Bag of Words (CBoW) Word2vec**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UuVr2IsaYhX"
      },
      "source": [
        "We have learn skip-gram model in tutorial. Now, we will explore another popular Word2Vec paradigm called Continuous Bag of Words (CBoW). *CBoW* offers faster processing and slightly better accuracy for common words compared to the *Skip-Gram*, which is more effective with rare words.\n",
        "\n",
        "**CBoW Structure**\n",
        "\n",
        "Below is the CBoW model architecture:\n",
        "\n",
        "![](https://i.ibb.co/StXTMFH/CBOW.png)\n",
        "\n",
        "In CBoW, the goal is to predict a target word from its surrounding context, represented by the sum of context vectors.\n",
        "\n",
        "We will leverage our understanding from the *Skip-Gram* model to implement *CBoW*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ENsl-sbox1m"
      },
      "source": [
        "1. **Batches Generations**\n",
        "**<font color=\"red\">[Task]</font>** : Implement the batch generator. **DONE**\n",
        "\n",
        "**Hint**: The generator should produce a input matrix `(batch_size, 2 * window_size)` containing context word indices and a target matrix `(batch_size)` with central word indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VNaP0uaU7T2-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nwe use yield at the end of for loop because of the next() function in the cell below\\n'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def make_cbow_batches_iter(contexts, window_size, batch_size):\n",
        "    # why word != 0? word[0] = kohinoor\n",
        "    central_words = np.array([word for word, context in contexts if len(context) == 2 * window_size and word != 0])\n",
        "    contexts = np.array([context for word, context in contexts if len(context) == 2 * window_size and word != 0])\n",
        "\n",
        "\n",
        "    batches_count = int(math.ceil(len(contexts) / batch_size))\n",
        "    print(f'central_words: {central_words.shape}, contexts: {contexts.shape}\\n')\n",
        "    # batch_size=32，那么就需要 12380个batch以后才完整地过了一遍数据集\n",
        "    print('Initializing batches generator with {} batches per epoch'.format(batches_count))\n",
        "\n",
        "    indices = np.arange(len(contexts))\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    for i in range(batches_count):\n",
        "      # 最后一个batch可能不够batch_size个数据, 取到最后一个indices\n",
        "      batch_begin, batch_end = i * batch_size, min((i + 1) * batch_size, len(contexts))\n",
        "      batch_indices = indices[batch_begin: batch_end]\n",
        "\n",
        "      # ------------------\n",
        "      # Write your implementation here.\n",
        "\n",
        "      contexts_batch = torch.LongTensor(contexts[batch_indices])\n",
        "      central_word_batch = torch.LongTensor(central_words[batch_indices])\n",
        "\n",
        "      batch = {'tokens': contexts_batch, 'labels': central_word_batch}\n",
        "\n",
        "      # yied: avoid producing all batch at once/help save ram usage, this is to improv effi\n",
        "      yield batch\n",
        "      # ------------------\n",
        "'''\n",
        "we use yield at the end of for loop because of the next() function in the cell below\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBF7xiik7ZaN"
      },
      "source": [
        "Check it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IVrQl8S4L9j",
        "outputId": "0d74297b-d2b9-49d7-e5b9-3d5fb533b721"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "central_words: (396154,), contexts: (396154, 4)\n",
            "\n",
            "Initializing batches generator with 12380 batches per epoch\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'tokens': tensor([[  58,   94,   48, 1362],\n",
              "         [1903,   23,   16,   55],\n",
              "         [  22,   57,  140,    2],\n",
              "         [  41, 2878,    0,   40],\n",
              "         [  12,    2,   13,  658],\n",
              "         [   3,  136,   17,   82],\n",
              "         [  82,    0,    0,    9],\n",
              "         [   9,    2,   16, 1300],\n",
              "         [   7, 2822,   65,  285],\n",
              "         [  13,  251,  341,  359],\n",
              "         [  51,  137,  825,   16],\n",
              "         [ 174,    8,    2,  279],\n",
              "         [   1,   49,   96,   82],\n",
              "         [1344,    7,  718,   15],\n",
              "         [  32,   12,   52, 1255],\n",
              "         [ 919,   37,    3,  439],\n",
              "         [   3,  101,   90,    1],\n",
              "         [1625,   16,  286, 1798],\n",
              "         [  31,    2,    0, 2535],\n",
              "         [  94,    2,    9,  678],\n",
              "         [3557, 6082,    2,  222],\n",
              "         [  23,  157,    2,  132],\n",
              "         [   3,    4, 1567,    9],\n",
              "         [5965, 2500,    4,   44],\n",
              "         [  19,  129,  567, 1077],\n",
              "         [  27,    2,   11,    7],\n",
              "         [  24,   56, 1962,    9],\n",
              "         [ 942,   34,   98, 1000],\n",
              "         [   2,   20,  501,    9],\n",
              "         [  23,   25,   14,   61],\n",
              "         [ 184,  273,  180,  789],\n",
              "         [  82,    7,  802, 1501]]),\n",
              " 'labels': tensor([   7,  280, 4004,   33,  634,   10,   13,  133,  107,    2,  198, 6020,\n",
              "           17,   48, 1526,    1,   46,  101, 1528,   20,    9,   33,  319,  895,\n",
              "            8, 1847,  172,    6,  321,  413,   13, 2025])}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "window_size = 2\n",
        "batch_size = 32\n",
        "\n",
        "batch = next(make_cbow_batches_iter(contexts, window_size=window_size, batch_size=batch_size))\n",
        "\n",
        "batch\n",
        "\n",
        "# check how is the training batch of CBoW model looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "all good so far, proceed.\n"
          ]
        }
      ],
      "source": [
        "assert isinstance(batch, dict)\n",
        "assert 'labels' in batch and 'tokens' in batch\n",
        "\n",
        "assert isinstance(batch['tokens'], torch.LongTensor)\n",
        "assert isinstance(batch['labels'], torch.LongTensor)\n",
        "\n",
        "assert batch['tokens'].shape == (batch_size, 2 * window_size)\n",
        "assert batch['labels'].shape == (batch_size,)\n",
        "\n",
        "# ======\n",
        "print('\\nall good so far, proceed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Here we attach the `make_skip_gram_batches_iter` from Tutorials for reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def make_skip_gram_batches_iter(contexts, window_size, num_skips, batch_size):\n",
        "\n",
        "    '''\n",
        "    params 含义\n",
        "    - contexts: 原始数据，假设为 (central_word<str>, context_words<tuple>) 的列表\n",
        "    - window_size: 上下文窗口半径（总窗口大小为 2 * window_size）\n",
        "    - num_skips: 每个中心词sample的上下文词的数量（每个中心词生成 num_skips 个样本）,2, 每个中心词在2*window_size的范围内随机采样num_skips个上下文词，因此你可以在最后生成的batch中观察到，tokens总是重复num_skips遍\n",
        "    - batch_size: 每个批次的总样本数, 32\n",
        "\n",
        "    assert 检查\n",
        "    - batch_size 必须是 num_skips 的倍数，保证每个中心词生成的样本能均匀分配到批次中。\n",
        "    - num_skips 不能超过总上下文词数量（2 * window_size）。\n",
        "\n",
        "    '''\n",
        "    assert batch_size % num_skips == 0\n",
        "    assert num_skips <= 2 * window_size\n",
        "\n",
        "    # len(context) == 2 : avoid context that is not full-size of 2*window_size\n",
        "    # typically words of start/end\n",
        "\n",
        "    # word != 0 : avoid using <unk> words as central word\n",
        "    central_words = [word for word, context in contexts if len(context) == 2 * window_size and word != 0]\n",
        "    contexts = [context for word, context in contexts if len(context) == 2 * window_size and word != 0]\n",
        "\n",
        "    batch_size = int(batch_size / num_skips)\n",
        "    batches_count = int(math.ceil(len(contexts) / batch_size))\n",
        "\n",
        "    print('Initializing batches generator with {} batches per epoch'.format(batches_count))\n",
        "\n",
        "    indices = np.arange(len(contexts))\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    for i in range(batches_count):\n",
        "        batch_begin, batch_end = i * batch_size, min((i + 1) * batch_size, len(contexts))\n",
        "        batch_indices = indices[batch_begin: batch_end]\n",
        "\n",
        "        # major difference from CBoW starts here\n",
        "        batch_data, batch_labels = [], []\n",
        "\n",
        "        for data_ind in batch_indices:\n",
        "            central_word, context = central_words[data_ind], contexts[data_ind]\n",
        "            \n",
        "            # sample num_skips amount of contexts words around central words\n",
        "            words_to_use = random.sample(context, num_skips)\n",
        "            batch_data.extend([central_word] * num_skips)\n",
        "            batch_labels.extend(words_to_use)\n",
        "        \n",
        "        # major difference ends\n",
        "        yield {\n",
        "            'tokens': torch.LongTensor(batch_data),\n",
        "            'labels': torch.LongTensor(batch_labels)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing batches generator with 24760 batches per epoch\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'tokens': tensor([ 592,  592,    3,    3,    6,    6,   24,   24, 2061, 2061,    9,    9,\n",
              "          773,  773,  528,  528,   21,   21,   66,   66,    8,    8,   70,   70,\n",
              "           11,   11,    8,    8,    2,    2, 1847, 1847]),\n",
              " 'labels': tensor([  83,    5,   34,  132,  763,   14,    1, 1621,   11,    0,    1, 2965,\n",
              "           15, 4108,   46,   56, 1925,   13,  478,   21,   20,  574,    5,   92,\n",
              "           30,    2,   30,   66,   35,    0,  169,  214])}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_skip_gram = next(make_skip_gram_batches_iter(contexts, window_size=2, num_skips=2, batch_size=32))\n",
        "\n",
        "batch_skip_gram\n",
        "\n",
        "# check how is the training data of skip-gram model looks like"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbKbZ_4E7T3U"
      },
      "source": [
        "2. **Model**\n",
        "**<font color=\"red\">[Task]</font>**: Build the `CBoWModel`.\n",
        "\n",
        "**Hint**: You need to implement the `forward` method based on the CBoW architecture. The context embedding is represented as the average of their context embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Mkawxwe77T3V"
      },
      "outputs": [],
      "source": [
        "# implement CBoW:\n",
        "class CBoWModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.out_layer = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # ------------------\n",
        "        # Write your implementation here.\n",
        "\n",
        "        embedded = self.embeddings(inputs)  # shape: (batch_size, 2*window_size, embedding_dim)\n",
        "\n",
        "        # averaging 4 contexts words (window_size = 2) vectors\n",
        "        mean_embedded = torch.mean(embedded, dim=1)  # (batch_size, embedding_dim)\n",
        "\n",
        "        outputs = self.out_layer(mean_embedded)  # (batch_size, vocab_size)\n",
        "\n",
        "        return outputs\n",
        "        # ------------------\n",
        "# here cbow and skipgram shares the same structure of nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHhTDxya7a3S"
      },
      "source": [
        "Check it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "nh_mNh__6lG2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "so far all good, proceed\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "line marked with 'cuda #' in case codes are tested in a cpu local machine, you can safely ignore and run\n",
        "'''\n",
        "# automatically adapt to different device.\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device(type='cpu')\n",
        "# cuda #1\n",
        "model = CBoWModel(vocab_size=len(word2index), embedding_dim=32)#.cuda()\n",
        "model.to(device)\n",
        "\n",
        "# cuda #2\n",
        "# outputs = model(batch['tokens'].cuda())\n",
        "outputs = model(batch['tokens'].to(device))\n",
        "\n",
        "# outputs = model(batch['tokens'])\n",
        "\n",
        "# cuda #3\n",
        "# no need to check if outputs is cuda.FloatTensor dtype\n",
        "# assert isinstance(outputs, torch.cuda.FloatTensor)\n",
        "# assert isinstance(outputs, torch.FloatTensor)\n",
        "\n",
        "assert outputs.shape == (batch_size, len(word2index))\n",
        "print('so far all good, proceed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmn56yki7T3a"
      },
      "source": [
        "3. **Training**\n",
        "**<font color=\"red\">[Task]</font>** : Train the CBoW.\n",
        "\n",
        "**Hint**: Consider referring to the training code of the previously mentioned *Skip-gram* model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzKLP9bs7T3b",
        "outputId": "ad273805-d00e-493c-a058-92bffec22cbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "central_words: (396154,), contexts: (396154, 4)\n",
            "\n",
            "Initializing batches generator with 3095 batches per epoch\n",
            "Epoch = 0, Step = 2999, Avg Loss = 6.4702, Time = 16.62s\n",
            "central_words: (396154,), contexts: (396154, 4)\n",
            "\n",
            "Initializing batches generator with 3095 batches per epoch\n",
            "Epoch = 1, Step = 2904, Avg Loss = 5.5461, Time = 32.94s\n",
            "central_words: (396154,), contexts: (396154, 4)\n",
            "\n",
            "Initializing batches generator with 3095 batches per epoch\n",
            "Epoch = 2, Step = 2809, Avg Loss = 5.2384, Time = 35.12s\n",
            "central_words: (396154,), contexts: (396154, 4)\n",
            "\n",
            "Initializing batches generator with 3095 batches per epoch\n",
            "Epoch = 3, Step = 2714, Avg Loss = 5.0436, Time = 33.21s\n",
            "central_words: (396154,), contexts: (396154, 4)\n",
            "\n",
            "Initializing batches generator with 3095 batches per epoch\n",
            "Epoch = 4, Step = 2619, Avg Loss = 4.9075, Time = 19.65s\n",
            "central_words: (396154,), contexts: (396154, 4)\n",
            "\n",
            "Initializing batches generator with 3095 batches per epoch\n",
            "Epoch = 5, Step = 2524, Avg Loss = 4.7910, Time = 29.10s\n",
            "central_words: (396154,), contexts: (396154, 4)\n",
            "\n",
            "Initializing batches generator with 3095 batches per epoch\n",
            "Epoch = 6, Step = 2429, Avg Loss = 4.7095, Time = 37.40s\n",
            "central_words: (396154,), contexts: (396154, 4)\n",
            "\n",
            "Initializing batches generator with 3095 batches per epoch\n",
            "Epoch = 7, Step = 2334, Avg Loss = 4.6365, Time = 33.45s\n",
            "central_words: (396154,), contexts: (396154, 4)\n",
            "\n",
            "Initializing batches generator with 3095 batches per epoch\n",
            "Epoch = 8, Step = 2239, Avg Loss = 4.5725, Time = 32.93s\n",
            "central_words: (396154,), contexts: (396154, 4)\n",
            "\n",
            "Initializing batches generator with 3095 batches per epoch\n",
            "Epoch = 9, Step = 2144, Avg Loss = 4.5143, Time = 30.04s\n"
          ]
        }
      ],
      "source": [
        "# Here are the hyperparameters you can adjust\n",
        "embedding_dim = 32\n",
        "learning_rate = 0.001\n",
        "epoch_num = 10 # default 5, can attempt more epochs num\n",
        "batch_size = 128\n",
        "\n",
        "# len(word2index) = 7226\n",
        "model = CBoWModel(len(word2index),embedding_dim)\n",
        "# Getting model to GPU\n",
        "# cuda #3 ===\n",
        "model.to(device)\n",
        "# loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# use Adam optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_every_nsteps = 3000 # report loss every 3000 steps\n",
        "total_loss = 0\n",
        "start_time = time.time()\n",
        "global_step = 0\n",
        "\n",
        "for ep in range(epoch_num):\n",
        "  for step, batch in enumerate(make_cbow_batches_iter(contexts, window_size=2, batch_size=batch_size)):\n",
        "      global_step += 1\n",
        "\n",
        "      # ------------------\n",
        "      # Write your implementation here.\n",
        "\n",
        "      # cuda #4 ====\n",
        "      inputs = batch['tokens'].to(device)  # contexts\n",
        "      labels = batch['labels'].to(device)  # central_words\n",
        "\n",
        "      # forward\n",
        "      outputs = model(inputs)  # shape: (batch_size, vocab_size)\n",
        "\n",
        "      # cal loss\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      # bp\n",
        "      optimizer.zero_grad()  # zero out grad every new batch\n",
        "      loss.backward()        # bp\n",
        "      optimizer.step()       # update weights\n",
        "      # ------------------\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      if global_step != 0 and global_step % loss_every_nsteps == 0:\n",
        "          print(\"Epoch = {}, Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(ep, step, total_loss / loss_every_nsteps,time.time() - start_time))\n",
        "\n",
        "          # reset diameter for next epochs\n",
        "          total_loss = 0\n",
        "          start_time = time.time()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcdVwk_gtiC8"
      },
      "source": [
        "**Obtaining word embeddings**\n",
        "\n",
        "Word embeddings are contained within the embeddings layer of the model. We just need to move them from the GPU to the CPU and convert them to a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "YRu-WeMbtivr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding size: (7226, 32)\n",
            "what:\n",
            " [ 0.96052194  0.05893503 -1.3039774  -0.88900465  0.677495    0.19581018\n",
            "  0.8522649   0.42557326  0.2250157  -0.7901772   0.07134378  1.7866141\n",
            "  0.50178045 -0.42438436  0.8282317  -0.68094814 -1.931252    0.6259549\n",
            "  0.4067984   1.0212533  -0.5563819   1.480883   -0.94625306  0.03295426\n",
            "  0.86704636 -1.0502287   0.38664585 -1.1708677   0.11243499 -1.7395645\n",
            "  1.154449   -0.61125165]\n",
            "is:\n",
            " [ 1.4916081  -1.3526965   0.8027263  -0.72304356  0.3714146  -0.68197644\n",
            " -2.487755   -2.1356645   0.7412488   0.2170881   2.030554   -1.1919757\n",
            "  1.7313095  -0.6281554  -1.902759    2.85017     0.253746    1.1202734\n",
            " -1.7669542  -0.73531795 -1.9330088   1.1174059  -1.718955   -0.13146304\n",
            " -1.762283    1.2027557  -2.6531742   2.3456078   0.5439756   0.7073023\n",
            "  3.780317    1.3381276 ]\n",
            "the:\n",
            " [-1.7554145   0.29734474  5.097264    2.0101798  -0.06181643 -1.794369\n",
            " -1.2909542   1.9649345  -0.3047359  -0.12137443 -1.235201    2.4657967\n",
            " -1.3481038   0.39234203 -0.21756071  0.86974037  0.01673725  4.2924957\n",
            " -1.1193606  -0.48889542  0.18732627 -1.4205573   1.0990207  -5.4046335\n",
            " -0.47877344  0.13305612  3.6237068  -0.39748105 -0.12638693  0.53067905\n",
            "  1.4416606  -1.0391207 ]\n"
          ]
        }
      ],
      "source": [
        "embeddings = model.embeddings.weight.data.cpu().numpy()\n",
        "# if you dont convert tensor from cuda to cpu, error:\n",
        "# TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\n",
        "print(f'embedding size: {embeddings.shape}')\n",
        "for i in range(len(embeddings[:3])):\n",
        "    print(f'{contexts_copy[i][0]}:\\n {embeddings[i]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-GyEW7s76-Q"
      },
      "source": [
        "**Testing Trained Word Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqRo0WHOtrpu"
      },
      "source": [
        "Let's check how adequate are similarities that the model learnt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjZ4Ki8RtXZH",
        "outputId": "d95dfd70-7d2e-4b7d-8271-b734a370c01f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "most similar words to 'my':\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['my',\n",
              " 'his',\n",
              " 'your',\n",
              " 'her',\n",
              " 'reader',\n",
              " 'their',\n",
              " 'someones',\n",
              " 'believing',\n",
              " 'tweets',\n",
              " 'pandora']"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def most_similar(embeddings, index2word, word2index, word):\n",
        "    word_emb = embeddings[word2index[word]]\n",
        "\n",
        "    similarities = cosine_similarity([word_emb], embeddings)[0]\n",
        "    top10 = np.argsort(similarities)[-10:]\n",
        "\n",
        "    return [index2word[index] for index in reversed(top10)]\n",
        "\n",
        "print(f'most similar words to \\'my\\':')\n",
        "most_similar(embeddings, index2word, word2index, 'my')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQW4PBdF96xC"
      },
      "source": [
        "**Visualization of our embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "5utoAUjltQZ6",
        "outputId": "dde46e17-9971-4034-c72a-e8bfa93692e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[t-SNE] Computing 91 nearest neighbors...\n",
            "[t-SNE] Indexed 100 samples in 0.001s...\n",
            "[t-SNE] Computed neighbors for 100 samples in 0.083s...\n",
            "[t-SNE] Computed conditional probabilities for sample 100 / 100\n",
            "[t-SNE] Mean sigma: 4.839828\n",
            "[t-SNE] KL divergence after 250 iterations with early exaggeration: 62.675255\n",
            "[t-SNE] KL divergence after 1000 iterations: 0.546376\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:261: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "        .bk-notebook-logo {\n",
              "            display: block;\n",
              "            width: 20px;\n",
              "            height: 20px;\n",
              "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
              "        }\n",
              "    </style>\n",
              "    <div>\n",
              "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
              "        <span id=\"aabde111-72b0-40a3-b958-5906a33f8ac3\">Loading BokehJS ...</span>\n",
              "    </div>\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n    // Clean up Bokeh references\n    if (id != null && id in Bokeh.index) {\n      Bokeh.index[id].model.document.clear();\n      delete Bokeh.index[id];\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim();\n            if (id in Bokeh.index) {\n              Bokeh.index[id].model.document.clear();\n              delete Bokeh.index[id];\n            }\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"aabde111-72b0-40a3-b958-5906a33f8ac3\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.2.0.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"aabde111-72b0-40a3-b958-5906a33f8ac3\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
            "application/vnd.bokehjs_load.v0+json": ""
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"a980662d-2df5-470f-9f5c-7c78d0486ebd\" data-root-id=\"p1090\" style=\"display: contents;\"></div>\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "(function(root) {\n  function embed_document(root) {\n  const docs_json = {\"c1fb591f-1ac3-46d7-8f4f-ce41c9435c83\":{\"version\":\"3.2.0\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1090\",\"attributes\":{\"height\":400,\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1091\"},\"y_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1092\"},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1099\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1100\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1097\"},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1124\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1087\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1088\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1089\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"0gOiv63cd77zac6/lzukP8XIw7/CIKo/mykJv5U4vj+LYd++FNQQQCvLRb9juJo/L7Iav34PCkBTsCO+FIN9v41YlT8R7P2/LBopPzvbPz8ug789pxv1P1Em9L5sI/6+KCmlv40wfL+CL0Y/ZX8rP5hBbL7yU6C/kB2RPxspSb9gr5a/tLAMQPtkoj6Kkhi/fkUVv2m01j0liyy/IS6cPA7KMr/XiC+/TYhyvyUtiT1+Wo6++6NVPxqh/D/987e+rLX5PxdbDr+rpgPAyNPIPiTBhz+RMfm+vQJNPmc+rj7zifc+U6SNvu/IAkAQk56//kyUP9euk7+405E/wCPgvi8pab/w+Ds9YczbP2m8BL6RcfG+jPqZPwT7UL6Efw4+L5Oev3P9qr8r7xg/EXxkvsuDfD+YPPs+q5IQPmHAOz6ojh8+f2yOPYo8l7/0Nam/qWEQP/mzXz+rOxA/NScKv6Q/Br8vhsS/jydKPnPKHz6rbju/h9a2Pl0NQr6tJO0+QEfcv+Vafz790JY9RBmSvg==\"},\"shape\":[100],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"gSZEvzkD0j8SArQ+WfuJvwtAJz6Mxn0/gC0GQL+3rz4FuHu/iScpv+9P1L9xZty/3TOjvp8VLr69v7G/gUimvmicmD+G548+mujlP3lKXr03kQNALpMqv7U7Y79YTja+HVeaPgw1rD4CG5m/NzBGv9C9q79dNME/Ggtxv+Ls4z5ARN++SHekvRN4kr8em/I/tuOwv011BECDcKA/iB0gvez9Or9g3O8+TGStPtXA3L/dMre/at00P6uzp75QQY6+iV6dvt11Fb1bp7a+ymUivpfu3705MzI/sfGYP9vJ5D4NUio/NVUzP4g9EL/BYqs/iUmDP8SEUz7hjnG/6Ku7P71air9l33K/zzFYP4nCnj4Y4Le/LHGgP7kQlT9JTEM/uHTJvml7v79ZCqQ/I1mvPwFiYL/sB+y+AWcNv7224j7dQ8w/FWARv4hjUz/ShWi+QdTFP/6nBz+YzcS/k2n9PpNhA8Cd2mA/DmNnv3WKV77GXXG/pm5kv8T2R74TKCU+54uMv/IXHb95iKY+mrTvvQ==\"},\"shape\":[100],\"dtype\":\"float32\",\"order\":\"little\"}],[\"color\",[\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\"]],[\"token\",[\"?\",\"the\",\"what\",\"is\",\"how\",\"i\",\"a\",\"to\",\"in\",\"do\",\"of\",\"are\",\"and\",\"can\",\"for\",\",\",\"you\",\"why\",\"it\",\"best\",\"my\",\"does\",\"on\",\"or\",\"which\",\"if\",\"be\",\"have\",\"with\",\"some\",\"'s\",\"that\",\".\",\"should\",\"get\",\"an\",\"from\",\"your\",\"india\",\")\",\"(\",\"who\",\"when\",\"like\",\"at\",\"people\",\"will\",\"good\",\"would\",\"there\",\"between\",\"as\",\"about\",\"``\",\"''\",\"not\",\"n't\",\"one\",\"did\",\"most\",\"we\",\"where\",\"was\",\"any\",\"by\",\"make\",\"way\",\"so\",\"after\",\"they\",\"quora\",\"life\",\":\",\"difference\",\"me\",\"this\",\"has\",\"know\",\"learn\",\"time\",\"their\",\"use\",\"many\",\"much\",\"someone\",\"money\",\"am\",\"all\",\"new\",\"think\",\"find\",\"work\",\"without\",\"become\",\"indian\",\"ever\",\"than\",\"start\",\"more\",\"better\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1125\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1126\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1121\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":10},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.25},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.25},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.25}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1122\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":10},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1123\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":10},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1098\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1111\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1112\"},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1113\",\"attributes\":{\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1114\",\"attributes\":{\"syncable\":false,\"level\":\"overlay\",\"visible\":false,\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"bottom_units\":\"canvas\",\"top_units\":\"canvas\",\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5}}}},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1115\"},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1116\"},{\"type\":\"object\",\"name\":\"HelpTool\",\"id\":\"p1117\"},{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1127\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":[[\"token\",\"@token\"]]}}],\"active_scroll\":{\"id\":\"p1112\"}}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1106\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1107\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1108\"},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1109\"}}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1101\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1102\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1103\"},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1104\"}}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1105\",\"attributes\":{\"axis\":{\"id\":\"p1101\"}}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1110\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1106\"}}}]}}]}};\n  const render_items = [{\"docid\":\"c1fb591f-1ac3-46d7-8f4f-ce41c9435c83\",\"roots\":{\"p1090\":\"a980662d-2df5-470f-9f5c-7c78d0486ebd\"},\"root_ids\":[\"p1090\"]}];\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    let attempts = 0;\n    const timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);",
            "application/vnd.bokehjs_exec.v0+json": ""
          },
          "metadata": {
            "application/vnd.bokehjs_exec.v0+json": {
              "id": "p1090"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import bokeh.models as bm, bokeh.plotting as pl\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "\n",
        "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
        "                 width=600, height=400, show=True, **kwargs):\n",
        "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
        "    output_notebook()\n",
        "\n",
        "    if isinstance(color, str):\n",
        "        color = [color] * len(x)\n",
        "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
        "\n",
        "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
        "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
        "\n",
        "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
        "    if show:\n",
        "        pl.show(fig)\n",
        "    return fig\n",
        "\n",
        "\n",
        "def get_tsne_projection(word_vectors):\n",
        "    tsne = TSNE(n_components=2, verbose=1)\n",
        "    return scale(tsne.fit_transform(word_vectors))\n",
        "\n",
        "\n",
        "def visualize_embeddings(embeddings, index2word, word_count):\n",
        "    word_vectors = embeddings[1: word_count + 1]\n",
        "    words = index2word[1: word_count + 1]\n",
        "\n",
        "    word_tsne = get_tsne_projection(word_vectors)\n",
        "    draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='blue', token=words)\n",
        "\n",
        "\n",
        "visualize_embeddings(embeddings, index2word, 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkidM73PdLva"
      },
      "source": [
        "## Task 2： Explore Word Embeddings with Word2Vec\n",
        "In this task, we shall explore the embeddings produced by word2vec. Please revisit the lecture slides or tutorials for more details on the word2vec algorithm. If you're feeling adventurous, challenge yourself and try reading the original [paper](https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf).\n",
        "\n",
        "Then run the following cells to load the word2vec vectors into memory. **Note**: This might take several minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "GI3v_EcWf9nW"
      },
      "outputs": [],
      "source": [
        "def load_word2vec():\n",
        "    \"\"\" Load GloVe Twitter Vectors\n",
        "        Return:\n",
        "            wv_from_bin: Pre-trained embeddings with 25 dimensions for 1.2M vocabulary.\n",
        "    \"\"\"\n",
        "    import gensim.downloader as api\n",
        "    wv_from_bin = api.load(\"glove-twitter-25\")\n",
        "    vocab = list(wv_from_bin.key_to_index.keys())  # Updated for Gensim 4.x\n",
        "    print(\"Loaded vocab size %i\" % len(vocab))\n",
        "    return wv_from_bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n7coG7kgHwU",
        "outputId": "cc0b46b5-6eed-4192-822f-3541eaf01a43"
      },
      "outputs": [],
      "source": [
        "# # -----------------------------------\n",
        "# # Run Cell to Load Word Vectors\n",
        "# # Note: This may take several minutes\n",
        "# # -----------------------------------\n",
        "\n",
        "wv_from_bin = load_word2vec()\n",
        "\n",
        "# save to local\n",
        "import pickle\n",
        "# # dump\n",
        "file_name = 'wv_from_bin.pkl'\n",
        "with open(file_name, 'wb') as f:\n",
        "    pickle.dump(wv_from_bin, f)\n",
        "\n",
        "# re-load\n",
        "with open(file_name, 'rb') as f:\n",
        "    wv_from_bin = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('<user>', 0)\n",
            "('.', 1)\n",
            "(':', 2)\n",
            "('rt', 3)\n",
            "(',', 4)\n",
            "('<repeat>', 5)\n",
            "('<hashtag>', 6)\n",
            "('<number>', 7)\n",
            "('<url>', 8)\n",
            "('!', 9)\n",
            "vector (embedding) size: 25\n",
            "exmaple of embedded vector: \n",
            "[[ 0.62415   0.62476  -0.082335  0.20101  -0.13741  -0.11431   0.77909\n",
            "   2.6356   -0.46351   0.57465  -0.024888 -0.015466 -2.9696   -0.49876\n",
            "   0.095034 -0.94879  -0.017336 -0.86349  -1.3348    0.046811  0.36999\n",
            "  -0.57663  -0.48469   0.40078   0.75345 ]\n",
            " [ 0.69586  -1.1469   -0.41797  -0.022311 -0.023801  0.82358   1.2228\n",
            "   1.741    -0.90979   1.3725    0.1153   -0.63906  -3.2252    0.61269\n",
            "   0.33544  -0.57058  -0.50861  -0.16575  -0.98153  -0.8213    0.24333\n",
            "  -0.14482  -0.67877   0.7061    0.40833 ]]\n"
          ]
        }
      ],
      "source": [
        "# we want to check the structure of 'wv_from_bin', but wv_from_bin.key_to_index is a huge dictionary, so we adapt \"yield\" to iter 10 random items from the dict (dict is not ordered)\n",
        "def list_10items_from_dict(dic):\n",
        "    for i in dic.items():\n",
        "        yield i\n",
        "\n",
        "items_from_dict = list_10items_from_dict(wv_from_bin.key_to_index)\n",
        "for i in range(10):\n",
        "    print(next(items_from_dict))\n",
        "\n",
        "print(f'vector (embedding) size: {wv_from_bin.vector_size}')\n",
        "print(f'exmaple of embedded vector: \\n{wv_from_bin.vectors[:2]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3ONN2P8z9yA"
      },
      "source": [
        "\n",
        "**Reducing dimensionality of Word2Vec Word Embeddings**\n",
        "\n",
        "Let's directly compare the word2vec embeddings to those of the co-occurrence matrix. Run the following cells to:\n",
        "\n",
        "- Put the 1.2 million word2vec vectors into a matrix M\n",
        "- Run reduce_to_k_dim (your Truncated SVD function) to reduce the vectors from 25-dimensional to 2-dimensional.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Cmx7pKhhh1Qj"
      },
      "outputs": [],
      "source": [
        "def get_matrix_of_vectors(wv_from_bin, required_words=['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'venezuela']):\n",
        "    \"\"\" Put the word2vec vectors into a matrix M.\n",
        "        Param:\n",
        "            wv_from_bin: KeyedVectors object; the 1.2 million word2vec vectors loaded from file\n",
        "        Return:\n",
        "            M: numpy matrix shape (num words, 300) containing the vectors\n",
        "            word2Ind: dictionary mapping each word to its row number in M\n",
        "    \"\"\"\n",
        "    import random\n",
        "    words = list(wv_from_bin.key_to_index.keys())\n",
        "    print(\"Shuffling words ...\")\n",
        "    random.shuffle(words)\n",
        "    words = words[:10000]\n",
        "    print(\"Putting %i words into word2Ind and matrix M...\" % len(words))\n",
        "    word2Ind = {}\n",
        "    M = []\n",
        "    curInd = 0\n",
        "    for w in words:\n",
        "        try:\n",
        "            M.append(wv_from_bin.word_vec(w))\n",
        "            word2Ind[w] = curInd\n",
        "            curInd += 1\n",
        "        except KeyError:\n",
        "            continue\n",
        "    for w in required_words:\n",
        "        try:\n",
        "            M.append(wv_from_bin.word_vec(w))\n",
        "            word2Ind[w] = curInd\n",
        "            curInd += 1\n",
        "        except KeyError:\n",
        "            continue\n",
        "    M = np.stack(M)\n",
        "    print(\"Done.\")\n",
        "    return M, word2Ind"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwG7p3MepE6P"
      },
      "source": [
        "**Implement reduce_to_k_dim**\n",
        "\n",
        "Construct a method that performs dimensionality reduction on the matrix to produce k-dimensional embeddings. Use SVD to take the top k components and produce a new matrix of k-dimensional embeddings.\n",
        "\n",
        "Note: All of numpy, scipy, and scikit-learn (sklearn) provide some implementation of SVD, but only scipy and sklearn provide an implementation of Truncated SVD, and only sklearn provides an efficient randomized algorithm for calculating large-scale Truncated SVD. So please use [sklearn.decomposition.TruncatedSVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html).\n",
        "\n",
        "**<font color=\"red\">[Task]</font>**: Complete reduce_to_k_dim function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ggHnG5EcidGm"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "def reduce_to_k_dim(M, k=2):\n",
        "    \"\"\" Reduce a co-occurrence count matrix of dimensionality (num_corpus_words, num_corpus_words to a matrix of dimensionality (num_corpus_words, k) using TruncatedSVD.\n",
        "\n",
        "    truncated svd check: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
        "        \n",
        "        Params:\n",
        "            M (numpy matrix of shape (num_corpus_words, num_corpus_words)): co-occurrence matrix of word counts\n",
        "            k (int): embedding size of each word after dimension reduction\n",
        "        Return:\n",
        "            M_reduced (numpy matrix of shape (num_corpus_words, k)): matrix of k-dimensional word embeddings.\n",
        "    \"\"\"\n",
        "    n_iters = 10  # Number of iterations for SVD\n",
        "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
        "\n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "    svd = TruncatedSVD(n_components=k, n_iter=n_iters, random_state=0)  # truncated svd\n",
        "    M_reduced = svd.fit_transform(M)  # dimensionality reduction\n",
        "\n",
        "    # ------------------\n",
        "    print(\"Done.\")\n",
        "    return M_reduced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "hDG1rO_wh2zP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shuffling words ...\n",
            "Putting 10000 words into word2Ind and matrix M...\n",
            "Done.\n",
            "Running Truncated SVD over 10010 words...\n",
            "Done.\n",
            "M_reduced: [[ 2.4653165  -0.01928564]\n",
            " [ 2.8954818   0.09360451]\n",
            " [ 3.0669835   1.3508444 ]\n",
            " ...\n",
            " [ 0.19509113  0.46137983]\n",
            " [ 1.1864849   0.37794757]\n",
            " [-2.094064   -0.9062391 ]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\xuguy\\AppData\\Local\\Temp\\ipykernel_24880\\3415616940.py:20: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
            "  M.append(wv_from_bin.word_vec(w))\n",
            "C:\\Users\\xuguy\\AppData\\Local\\Temp\\ipykernel_24880\\3415616940.py:27: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
            "  M.append(wv_from_bin.word_vec(w))\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------\n",
        "# Run Cell to Reduce 25-Dimensinal Word Embeddings to k Dimensions\n",
        "# Note: This may take several minutes\n",
        "# -----------------------------------------------------------------\n",
        "import numpy as np\n",
        "M, word2Ind = get_matrix_of_vectors(wv_from_bin)\n",
        "M_reduced = reduce_to_k_dim(M, k=2)\n",
        "print(f'M_reduced: {M_reduced}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0fGsfm-oZG7"
      },
      "source": [
        "**Here is a helper function to plot a set of 2D vectors in 2D space. For graphs, we will use Matplotlib (plt).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "bMXt_-QPn0C0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_embeddings(M_reduced, word2Ind, words):\n",
        "    \"\"\" Plot in a scatterplot the embeddings of the words specified in the list \"words\".\n",
        "        Include a label next to each point.\n",
        "\n",
        "        Params:\n",
        "            M_reduced (numpy matrix of shape (number of unique words in the corpus, k)): matrix of k-dimensional word embeddings\n",
        "            word2Ind (dict): dictionary that maps word to indices for matrix M\n",
        "            words (list of strings): words whose embeddings we want to visualize\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for word in words:\n",
        "        if word in word2Ind:\n",
        "            idx = word2Ind[word]\n",
        "            x, y = M_reduced[idx, 0], M_reduced[idx, 1]\n",
        "            plt.scatter(x, y, marker='o', color='blue')\n",
        "            plt.text(x + 0.02, y + 0.02, word, fontsize=9)\n",
        "        else:\n",
        "            print(f\"Word '{word}' not found in word2Ind dictionary.\")\n",
        "\n",
        "    plt.title(\"Word Embeddings Visualization\")\n",
        "    plt.xlabel(\"Dimension 1\")\n",
        "    plt.ylabel(\"Dimension 2\")\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBbtacQhorHe"
      },
      "source": [
        "### 2.1: Word2Vec Plot Analysis\n",
        "Run the cell below to plot the 2D word2vec embeddings for ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'venezuela'].\n",
        "\n",
        "What clusters together in 2-dimensional embedding space? What doesn't cluster together that you might think should have? How is the plot different from the one generated earlier from the co-occurrence matrix?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "usSSq_x3llW6",
        "outputId": "2a9b872b-b7c1-4956-8db7-17b1aa8211e6"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "'\\ndue to the randomness of truncated SVD, the plot will be different for each run of this cell\\n'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'venezuela']\n",
        "plot_embeddings(M_reduced, word2Ind, words)\n",
        "\n",
        "'''\n",
        "due to the randomness of truncated SVD, the plot will be different for each run of this cell\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swps7Nsqo0Q5"
      },
      "source": [
        "**<font color=\"red\">[Task]</font>**:Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. What clusters together in 2-dimensional embedding space?\n",
        "   - oil/patro production countries, like **ecuador**, **kuwait** etc, cluster together with **oil** and **patro**\n",
        "2. What doesn't cluster together that you might think should have? \n",
        "   - **venezuela** should cluster with those mentioned above because **venuzuela** is a oil production country so it is closely related to **oil** or **patro**, but it doesn't\n",
        "3. How is the plot different from the one generated earlier from the co-occurrence matrix?\n",
        "   - ? wdym co-occurrence matrix? we have never dealed with co-occurence matrix before in this hw. do you mean the plot of matrix generated by cbow(Word2Vec)?\n",
        "   - difference of the matrix generated before and this one: they use different dimension-reduction algo, the one before use t-SNE from sklearn, this one use truncated-svd. can't see any significant difference from these 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mbzLnAwdOth"
      },
      "source": [
        "### 2.2 Polysemous Words\n",
        "Find a [polysemous](https://en.wikipedia.org/wiki/Polysemy) word (for example, \"leaves\" or \"scoop\") such that the top-10 most similar words (according to cosine similarity) contains related words from both meanings. For example, \"leaves\" has both \"turns\" and \"ground\" in the top 10, and \"scoop\" has both \"buckets\" and \"pops\". You will probably need to try several polysemous words before you find one. Please state the polysemous word you discover and the multiple meanings that occur in the top 10. Why do you think many of the polysemous words you tried didn't work?\n",
        "\n",
        "Note: You should use the wv_from_bin.most_similar(word) function to get the top 10 similar words. This function ranks all other words in the vocabulary with respect to their cosine similarity to the given word. For further assistance please check the GenSim [documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.most_similar)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQdi-3zJ3JSt",
        "outputId": "03a46118-9ca5-404d-ddc3-3da09838010c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['follow', 'people', 'really', 'please', 'twitter', 'hahaha', 'someone', 'always', 'because', 'thanks']\n"
          ]
        }
      ],
      "source": [
        "# ------------------\n",
        "# Write your polysemous word exploration code here.\n",
        "'''\n",
        "思路：1. 筛选出长度大于6的词，这些长词大部分都是名词、形容词、副词，这些词才好判断词义 2. 重点考察名词、形容词\n",
        "'''\n",
        "# 获取vocab\n",
        "vocabList = list(wv_from_bin.key_to_index.keys())\n",
        "len(vocabList) # 1193514\n",
        "\n",
        "def find_alpha(vocab, word_length=4):\n",
        "    wordsAlpha = []\n",
        "    # 找出前1000词中，由纯字母构成的词：str.isalpha()\n",
        "    for word in vocab[:1000]:\n",
        "        if word.isalpha() and len(word) >=word_length:\n",
        "            wordsAlpha.append(word)\n",
        "    return wordsAlpha\n",
        "# 筛选出长度大于或等于5的词，因为大多数非形容词长度都比较小\n",
        "alpha = find_alpha(vocabList[:1000], word_length = 6)\n",
        "\n",
        "# 打印出前10个看看\n",
        "print(alpha[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1pUnbtZq-ST",
        "outputId": "a09e8bbe-42fc-4fd9-851e-5deb8876e2b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing word: hard\n",
            "Top 10 similar words: \n",
            "['even', 'bad', 'fall', 'shit', 'its', 'harder', 'it', 'nothing', 'way', 'fast']\n",
            "\n",
            "Testing word: summit\n",
            "Top 10 similar words: \n",
            "['conference', 'alliance', 'convention', 'international', 'district', 'academy', 'development', 'chamber', 'arts', 'center']\n",
            "\n",
            "Testing word: tomorrow\n",
            "Top 10 similar words: \n",
            "['saturday', 'monday', 'friday', 'tonight', 'thursday', 'wednesday', 'today', 'soon', 'tuesday', 'coming']\n",
            "\n",
            "Testing word: little\n",
            "Top 10 similar words: \n",
            "['look', 'girl', 'like', 'pretty', 'kid', 'big', 'old', 'my', 'lady', 'one']\n",
            "\n",
            "Testing word: believe\n",
            "Top 10 similar words: \n",
            "['remember', 'never', 'one', 'wanted', 'forget', 'we', 'see', 'let', 'rush', 'meet']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 给定候选词，找出每个候选词的10个most_similar words，然后人工判断polysemous words\n",
        "def polysemous(candidate_words):\n",
        "\n",
        "    for word in candidate_words:\n",
        "        try:\n",
        "            top_10_similar = wv_from_bin.most_similar(word, topn=10)\n",
        "            top_10_words = [w for w, _ in top_10_similar]\n",
        "\n",
        "            # Check if the top-10 similar words contain evidence of multiple meanings\n",
        "            # (e.g., related words from both senses)\n",
        "            # This is a heuristic and may need adjustment based on the word.\n",
        "            if len(set(top_10_words)) >= 2:  # At least two distinct meanings\n",
        "                print(f\"Testing word: {word}\")\n",
        "                print(f\"Top 10 similar words: \\n{top_10_words}\\n\")\n",
        "\n",
        "        except KeyError:\n",
        "            print('words not found in vocab')\n",
        "            continue\n",
        "\n",
        "# 随意选择了一些候选词：\n",
        "candidate_words = ['hard', 'summit' ,'tomorrow', 'little', 'believe']\n",
        "polysemous(candidate_words)\n",
        "\n",
        "# ------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIiwx3NLrHnV"
      },
      "source": [
        "**<font color=\"red\">[Task]</font>**:Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. state the polysemous word you discover and the multiple meanings that occur in the top 10. \n",
        "   - **hard** has similar words **bad**, **nothing** etc, they are polysemous word of **hard**\n",
        "   - check above cell outputs for details.\n",
        "2. Why do you think many of the polysemous words you tried didn't work?\n",
        "   - i dont't get it, what do you want to ask? all the words i tried have polysemous words, so they all work\n",
        "   - if you are asking **why some of the words from the top 10 most similar might not be a polysemous word?**: do you mean why there are totally **irrelevant** words like **shit**, **it** appear in the top 10 most similar to **hard**, it may be because\n",
        "     - the CBoW model(as well as skip-gram) is trained by predicting a word given its context words, so words that frequently appear in similar contexts may have similar embeddings, even if they are unrelated in meaning.\n",
        "     - In a high-dimensional vector space, there can be accidental closeness between words that do not have a meaningful relationship.\n",
        "     - Training data have bias and may not have sufficient context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8vhD0k0dQ-z"
      },
      "source": [
        "### 2.3: Synonyms & Antonyms\n",
        "\n",
        "When considering Cosine Similarity, it's often more convenient to think of Cosine Distance, which is simply 1 - Cosine Similarity.\n",
        "\n",
        "Find three words (w1,w2,w3) where w1 and w2 are synonyms and w1 and w3 are antonyms, but Cosine Distance(w1,w3) < Cosine Distance(w1,w2). For example, w1=\"happy\" is closer to w3=\"sad\" than to w2=\"cheerful\".\n",
        "\n",
        "Once you have found your example, please give a possible explanation for why this counter-intuitive result may have happened.\n",
        "\n",
        "You should use the the wv_from_bin.distance(w1, w2) function here in order to compute the cosine distance between two words. Please see the GenSim documentation for further assistance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "Wli3CumGs50-",
        "outputId": "bd54a131-3f87-4813-e5cc-5c145bdb19f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Synonyms hard, difficult have cosine distance: 0.24550330638885498\n",
            "Antonyms hard, easy have cosine distance: 0.1183176040649414\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\nSynonyms hard, difficult have cosine distance: 0.24550330638885498\\nAntonyms hard, easy have cosine distance: 0.1183176040649414\\n'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# ------------------\n",
        "# Write your synonym & antonym exploration code here.\n",
        "\n",
        "# example\n",
        "w1 = \"hard\"\n",
        "w2 = \"difficult\"\n",
        "w3 = \"easy\"\n",
        "w1_w2_dist = wv_from_bin.distance(w1, w2)\n",
        "w1_w3_dist = wv_from_bin.distance(w1, w3)\n",
        "\n",
        "print(\"Synonyms {}, {} have cosine distance: {}\".format(w1, w2, w1_w2_dist))\n",
        "print(\"Antonyms {}, {} have cosine distance: {}\".format(w1, w3, w1_w3_dist))\n",
        "\n",
        "'''\n",
        "Synonyms hard, difficult have cosine distance: 0.24550330638885498\n",
        "Antonyms hard, easy have cosine distance: 0.1183176040649414\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZV7c7_4s9Is"
      },
      "source": [
        "**<font color=\"red\">[Task]</font>**:Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Once you have found your example, please give a possible explanation for why this counter-intuitive result may have happened.\n",
        "   - found one set of (w1,w2,w3): (hard, difficult, easy), **easy** is antonym to **hard**, but it has smaller cos-distance to **hard** than **difficult**, which is a synonym of **hard**\n",
        "   - **possible explaination**: the CBoW model(as well as skip-gram) is trained by predicting a word given its context words, so words that frequently appear in similar contexts may have similar embeddings, even if they are opposite in meaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4G2X-TTpzwz"
      },
      "source": [
        "## Task 3: Utilize Word Embeddings\n",
        "\n",
        "Guess, you've seen such pictures already:  \n",
        "\n",
        "![Embeddings Relations](https://www.tensorflow.org/images/linear-relationships.png)\n",
        "*Source: [Tensorflow tutorial on Vector Representations of Words](https://www.tensorflow.org/tutorials/representation/word2vec)*\n",
        "\n",
        "In the first image, we observe the intricate relationships encoded within the word embeddings space. This encompasses various dimensions like gender differences (male-female) or verb tenses.\n",
        "\n",
        "**Interactive Exploration**\n",
        "\n",
        "To delve deeper and interactively explore these relationships, check out these resources:\n",
        "- [Word Vector Demo](http://bionlp-www.utu.fi/wv_demo/)\n",
        "- [Word2Viz](https://lamyiowce.github.io/word2viz/)\n",
        "\n",
        "These tools offer a playful yet insightful experience, allowing you to grasp the nuances and capabilities of word embeddings.\n",
        "\n",
        "**Our task point**\n",
        "\n",
        "Our focus will be on utilizing [gensim](https://radimrehurek.com/gensim/), a well-regarded Python library for word embeddings. Gensim makes it effortless to work with and leverage the power of word embeddings in various applications.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIvBhh71WIeS"
      },
      "source": [
        "### **3.1 Use Pretrained Embeddings**\n",
        "Base on gensim, we can easily use a well-pretrained embeddings model. There are a number of such models in <font color=\"blue\">gensim</font>, you can call `api.info()` to get the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OEazfh1s9eki"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "model = api.load('glove-twitter-25')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPQxqjIGZxt_"
      },
      "source": [
        "**use word embedidngs with gensim**\n",
        "\n",
        "Yay, we have loaded well-built word embedings models, now let's learn how to use it.\n",
        "\n",
        "1. To get word's vector, well, call `get_vector`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uF6iF6A9uGQ",
        "outputId": "dc86d016-e921-457e-e7d8-8ba4c712db03"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.47841 ,  0.39537 , -0.3216  ,  0.58639 , -0.48316 ,  0.11402 ,\n",
              "        1.3829  , -0.86081 , -0.81769 , -0.075026, -0.77716 ,  0.58212 ,\n",
              "       -5.2756  , -0.54024 ,  0.39019 ,  0.3941  ,  0.32682 , -0.7274  ,\n",
              "        0.49747 , -0.88427 , -0.062516,  0.035716, -0.28677 ,  0.64153 ,\n",
              "       -0.574   ], dtype=float32)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.get_vector('anything')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiXwAZTsaHCf"
      },
      "source": [
        "2. To get most similar words for the given one :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57uH83XZaI6p",
        "outputId": "81eb1f81-ad2d-48ca-8b0c-966899b70a5f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('meat', 0.9616428017616272),\n",
              " ('corn', 0.961062490940094),\n",
              " ('cheese', 0.9532766342163086),\n",
              " ('noodles', 0.9493104815483093),\n",
              " ('soup', 0.9440536499023438),\n",
              " ('egg', 0.9418218731880188),\n",
              " ('milk', 0.941437304019928),\n",
              " ('chicken', 0.9398934841156006),\n",
              " ('beans', 0.9390753507614136),\n",
              " ('toast', 0.936586856842041)]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.most_similar('bread')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rtyp__uQaVcR"
      },
      "source": [
        "3. Analogies with word embeddings\n",
        "\n",
        "It can do such magic (`woman` + `grandfather` - `man`) :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9igEyCm6aqfU",
        "outputId": "f6653311-6c79-4bc5-c28e-e1500e1588bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "top 10 most similar words: \n",
            "[('grandmother', 0.878795325756073), ('deceased', 0.8755999803543091), ('grandson', 0.8732503652572632), ('granddaughter', 0.8626090884208679), ('mother-in-law', 0.8423668742179871), ('stabs', 0.8338027596473694), ('adopted', 0.8286494612693787), ('marries', 0.825094997882843), ('brother-in-law', 0.8129834532737732), ('fiancee', 0.8020613193511963)]\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to answer the analogy -- man : grandfather :: woman : x\n",
        "most_si = model.most_similar(positive=['woman', 'grandfather'], negative=['man'])\n",
        "print(f'top 10 most similar words: \\n{most_si}')\n",
        "\n",
        "# simply pick the most similar one, which is 'grandmother' to be our analogy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwCkJSNraruT"
      },
      "source": [
        "And this too:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4t94HZXa1vd",
        "outputId": "a57281c9-f5f7-426a-cc68-94aae8e34a33"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('gfx', 0.8166244029998779),\n",
              " ('realtor', 0.7994468808174133),\n",
              " ('promoters', 0.7922900319099426),\n",
              " ('promoter', 0.7778065800666809),\n",
              " ('recruiter', 0.7722606658935547),\n",
              " ('digg', 0.7702906727790833),\n",
              " ('sfi', 0.7655168771743774),\n",
              " ('chefs', 0.7650213837623596),\n",
              " ('smallbusiness', 0.7634385824203491),\n",
              " ('realestate', 0.7535584568977356)]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.most_similar([model.get_vector('coder') - model.get_vector('brain') + model.get_vector('money')])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c23cwWhKvtXp"
      },
      "source": [
        "That is, who is like coder, with money and without brains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a15dwaha36x"
      },
      "source": [
        "**<font color=\"red\">[Task]</font>** : Run an interesting analogy example\n",
        "\n",
        "**Hint**: Similar to (`woman` + `grandfather` - `man`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0D04rXsa_al",
        "outputId": "4120fdf3-527c-4b33-b7d8-9d577679e969"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "example #1: mother: queen :: father: king\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('father', 0.9109837412834167),\n",
              " ('called', 0.894909679889679),\n",
              " ('child', 0.8890705704689026),\n",
              " ('said', 0.881404459476471),\n",
              " ('woman', 0.8805525302886963),\n",
              " ('told', 0.8771608471870422),\n",
              " ('wife', 0.8754846453666687),\n",
              " ('daughter', 0.8657993078231812),\n",
              " ('dad', 0.8648059964179993),\n",
              " ('heard', 0.862002432346344)]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ------------------\n",
        "# Write your implementation here.\n",
        "\n",
        "# my answer:\n",
        "print('example #1: mother: queen :: father: king')\n",
        "wv_from_bin.most_similar(positive=['king', 'mother'],negative=['queen'])\n",
        "\n",
        "# ------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkzzGBhO4z_z",
        "outputId": "fe857d30-4fbe-4d3b-fce1-bcf91407520a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "example #2: have: food :: shoes: wore\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('wore', 0.8886487483978271),\n",
              " ('wearing', 0.8869256973266602),\n",
              " ('wear', 0.8861392736434937),\n",
              " ('shirt', 0.8839967846870422),\n",
              " ('pair', 0.8752208352088928),\n",
              " ('wears', 0.8669461607933044),\n",
              " ('shoes', 0.8646951913833618),\n",
              " ('both', 0.8637152314186096),\n",
              " ('tie', 0.8606826066970825),\n",
              " ('won', 0.8483160138130188)]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ------------------\n",
        "# Write your implementation here.\n",
        "\n",
        "# my answer:\n",
        "print('example #2: have: food :: shoes: wore')\n",
        "wv_from_bin.most_similar(wv_from_bin.get_vector('have')-wv_from_bin.get_vector('food')+wv_from_bin.get_vector('shoes'))\n",
        "\n",
        "\n",
        "# ------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szS69OEObDzy"
      },
      "source": [
        "### **3.2 Finding the Most Similar Sentence**\n",
        "\n",
        "In this section, we present a method for sentence retrieval based on word embeddings.\n",
        "\n",
        "The key point is to construct *sentence embeddings*. The simplest method to obtain a sentence embedding is by averaging the embeddings of the words within the sentence.\n",
        "\n",
        "*You are probably thinking, 'What a dumb idea, why on earth the average of embedding should contain any useful information'. Well, check [this paper](https://arxiv.org/pdf/1805.09843.pdf).*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPJdCsjtfDxJ"
      },
      "source": [
        "1. Get Sentence Embedding\n",
        "\n",
        "**<font color=\"red\">[Task]</font>** : Implement a function to compute sentence embeddings.\n",
        "\n",
        "**Hint**: Tokenize and lowercase the texts. Calculate the mean embedding for words with known embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "collapsed": true,
        "id": "0LPw1fRg1GaW",
        "outputId": "3b6d5e6f-5a06-4d81-d04a-0c74ec6ffe6f"
      },
      "outputs": [],
      "source": [
        "# sentence embedding:\n",
        "# import gensim.downloader as api\n",
        "# from nltk.tokenize import word_tokenize\n",
        "\n",
        "# model = api.load('glove-twitter-25')\n",
        "\n",
        "def get_sentence_embedding(model, sentence):\n",
        "    \"\"\" Calcs sentence embedding as a mean of known word embeddings in the sentence.\n",
        "    If all the words are unknown, returns zero vector.\n",
        "    :param model: KeyedVectors instance\n",
        "    :param sentence: str or list of str (tokenized text)\n",
        "    \"\"\"\n",
        "    embedding = np.zeros([model.vector_size], dtype='float32')\n",
        "\n",
        "    if isinstance(sentence, str):\n",
        "        words = word_tokenize(sentence.lower())\n",
        "    else:\n",
        "        words = sentence\n",
        "\n",
        "    sum_embedding = np.zeros([model.vector_size], dtype='float32')\n",
        "    words_in_model = 0\n",
        "\n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "    vocabList = list(model.key_to_index.keys())\n",
        "    for word in words:\n",
        "        if word in vocabList: # NOTICE\n",
        "            sum_embedding += model.get_vector(word)\n",
        "            words_in_model +=1\n",
        "    # make sure sentence embedding has at least 1 word:\n",
        "    if words_in_model > 0:\n",
        "        embedding = sum_embedding/words_in_model\n",
        "\n",
        "    return embedding\n",
        "    # ------------------\n",
        "\n",
        "\n",
        "# 注意，原题目中的vector = get_sentence_embedding(model, \"I'm very sure. This never happened to me before...\")\n",
        "# assert vector.shape == (model.vector_size,)\n",
        "\n",
        "# '...' 会被tokenize 为'...'，我们的vocab里不存在这个东西，因此# NOTICE那一行的判断如果判断为False，sum_embedding将跳过vocab中没有的词，进行下一个\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FbZbKPI1OZf"
      },
      "source": [
        "Check it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5phpDHEcdK9",
        "outputId": "8ba49932-8226-4ab6-8b85-dbbcf61eabf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.04355327  0.13398808  0.16726881 -0.21951303 -0.53978294  0.01005662\n",
            "  1.4096314   0.05043383 -0.6068709   0.07903273 -0.48138705  0.06502508\n",
            " -4.9984365  -0.08382546  0.06057001  0.13585638  0.3144989  -0.27905494\n",
            " -0.39448598 -0.68288624  0.23462509  0.11004109  0.1565229   0.4441222\n",
            " -0.06729318]\n",
            "all good, proceed.\n"
          ]
        }
      ],
      "source": [
        "vector = get_sentence_embedding(model, \"I'm very sure. This never happened to me before...\")\n",
        "assert vector.shape == (model.vector_size,)\n",
        "print(vector)\n",
        "print('all good, proceed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-8wclbCdTfw"
      },
      "source": [
        "2. **Building the Index**\n",
        "\n",
        "With our method ready, we can now embed all sentences in our corpus for retrieval purposes. In this case, we use data from Quora, sampling 1000 entries randomly, and converting them into sentence embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "P6B2c-bJdCrB"
      },
      "outputs": [],
      "source": [
        "quora_data = pd.read_csv('train.csv')\n",
        "\n",
        "# corpus len=997, because there may be duplicate question\n",
        "# also not that we randomly sample(1000) from dataset, so corpus will differ\n",
        "corpus = list(quora_data.sample(1000)[['question1']].question1.replace(np.nan, '', regex=True).unique())\n",
        "\n",
        "# text_vector.shape (997, 25)\n",
        "# might take ~30s to run this line\n",
        "text_vectors = np.array([get_sentence_embedding(model, sentence) for sentence in corpus])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NXrJ-R8leSWR",
        "outputId": "c77a670d-dda7-4a23-ba8a-d4d0e0671a01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'How do I play Pokémon GO in Korea?'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71rQjmmK6fto",
        "outputId": "6c226623-8b1f-47e9-e33a-6843ac93974f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.3888489 ,  0.16029921,  0.19582754, -0.4084578 , -0.37283242,\n",
              "       -0.02436   ,  0.91520226,  0.29946578,  0.20591444, -0.03248673,\n",
              "       -0.2812218 , -0.14971478, -4.4854336 , -0.41685486,  0.0563231 ,\n",
              "        0.05395163, -0.077271  , -0.34662554, -0.40603998, -0.25013858,\n",
              "       -0.23563528,  0.21580285, -0.0303921 , -0.14689223,  0.19788776],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check also:\n",
        "text_vectors[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiMtj5dXebbY"
      },
      "source": [
        "3. **Search**\n",
        "\n",
        "Now we are able perform search of the nearest neighbours to the given sentences in our base!\n",
        "\n",
        "\n",
        "We'll use cosine similarity of two vectors:\n",
        "$$\\text{cosine\\_similarity}(x, y) = \\frac{x^{T} y}{||x||\\cdot ||y||}$$\n",
        "\n",
        "*It's not a [distance](https://www.encyclopediaofmath.org/index.php/Metric) strictly speaking but we still can use it to search for the sentence vectors.*\n",
        "\n",
        "**<font color=\"red\">[Task]</font>** : IImplement the following function.\n",
        "\n",
        "**Hint:** Calc the similarity between `query` embedding and `text_vectors` using `cosine_similarity` function. Find `k` vectors with highest scores and return corresponding texts from `texts` list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "ynEJW6E7eg0c"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def find_nearest(model, text_vectors, texts, query, k=10):\n",
        "    query_vec = get_sentence_embedding(model, query)\n",
        "\n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "    sentence_embeddings = get_sentence_embedding(model, query)\n",
        "\n",
        "    # squeeze() to reshape c from (1, n) to (n,)\n",
        "    cos_sim = cosine_similarity(sentence_embeddings.reshape(1, -1), text_vectors).squeeze()\n",
        "\n",
        "    # the bigger cosine_similarity the more similar\n",
        "    # so sort c in descenet order: -1*cos_sim\n",
        "    top_k_ind =((-cos_sim).squeeze().argsort())[:k]\n",
        "\n",
        "    # convert indices to question text\n",
        "    top_k_q = [corpus[q] for q in top_k_ind]\n",
        "    print(f'query: {query}')\n",
        "    return top_k_q\n",
        "    # ------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is8SoYmHkQo5"
      },
      "source": [
        "Check it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49s4zB1OjXd4",
        "outputId": "233bf30e-b20d-443f-aae1-e9f37a527628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "query: What's your biggest regret in life?\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['What is the turning point in your life?',\n",
              " 'What should the purpose of your life?',\n",
              " 'What is the most painful/humiliating punishment you ever received in your life?',\n",
              " 'What are the embarrassing moment of your life?',\n",
              " 'What does it mean when you keep having the same dream over and over of the same dead person?',\n",
              " 'Is it good to have sex before a marriage?',\n",
              " 'In the end, what seems to be the most important things in life?',\n",
              " 'What is the purpose of life?',\n",
              " \"I have failed the JEE Mains even after taking a drop. My dad has done a lot of investment in this and my relatives seek moments like this to piss me off. I'm very depressed right now. How do I overcome this?\",\n",
              " 'What are the basic things that one should keep in mind before starting a start up?']"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "find_nearest(model, text_vectors, corpus, \"What's your biggest regret in life?\", k=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**we can see from the top10 most similar questions to the questioned one, they are indeed very similar: all of the topic of life**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA5CwGV8jU5_"
      },
      "source": [
        "### **Bias of Word Embeddings**\n",
        "\n",
        "It's important to be cognizant of the biases (gender, race, sexual orientation etc.) implicit in our word embeddings. Bias can be dangerous because it can reinforce stereotypes through applications that employ these models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdIUrmnJxGvL"
      },
      "source": [
        "Here's an example showing word embeddings biases on gender:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWngJZCWxduU",
        "outputId": "f0552962-e4f1-40e9-9884-f98296c87712"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('maths', 0.7983576059341431), ('basis', 0.7973601818084717), ('humør', 0.7948898673057556), ('cert', 0.7902684211730957), ('mulig', 0.7874146103858948), ('spændende', 0.7728654742240906), ('dårligt', 0.7700907588005066), ('latter', 0.7676339745521545), ('noget', 0.7676041722297668), ('vet', 0.7675378918647766)]\n",
            "\n",
            "[('representation', 0.871566116809845), ('encourages', 0.8626720309257507), ('empowering', 0.8612703084945679), ('intellectual', 0.8564386963844299), ('influences', 0.8559868931770325), ('ethical', 0.8550472259521484), ('affairs', 0.8541139960289001), ('behaviors', 0.8481355309486389), ('advocacy', 0.8439522385597229), ('critic', 0.8406821489334106)]\n"
          ]
        }
      ],
      "source": [
        "print(model.most_similar(positive=['man', 'profession'], negative=['woman']))\n",
        "print()\n",
        "print(model.most_similar(positive=['woman', 'profession'], negative=['man']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpFDY9BByDBj"
      },
      "source": [
        "**<font color=\"red\">[Task]</font>** Identify an example of bias.\n",
        "\n",
        "**Hint:** Consider providing an example from perspectives such as race or sexual orientation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "np_RQalnx0Bb",
        "outputId": "5739ce75-b39e-4121-b08b-5fc109af822c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('red', 0.9298717975616455), ('grey', 0.8770253658294678), ('gold', 0.8569556474685669), ('golden', 0.8560071587562561), ('green', 0.8503260612487793), ('silver', 0.8307074904441833), ('black', 0.8287413716316223), ('yellow', 0.8248174786567688), ('iron', 0.8127517104148865), ('series', 0.812469482421875)]\n"
          ]
        }
      ],
      "source": [
        "# ------------------\n",
        "# Write your implementation here.\n",
        "\n",
        "# example 1: gender and color bias 1\n",
        "print(model.most_similar(positive=['he', 'blue'], negative=['she']))\n",
        "# ------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnkDqcWb7JJz",
        "outputId": "b3e58dfb-f8c3-4f7c-a9c1-7569ef53d29e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('white', 0.9437015652656555), ('black', 0.933352530002594), ('blue', 0.9333339929580688), ('purple', 0.9230028390884399), ('yellow', 0.9003236889839172), ('green', 0.8979281187057495), ('pink', 0.888805627822876), ('dark', 0.8869737982749939), ('diamond', 0.8821786642074585), ('shoes', 0.8815739750862122)]\n"
          ]
        }
      ],
      "source": [
        "# example 2: gender and color bias 1\n",
        "print(model.most_similar(positive=['she', 'red'], negative=['he']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYaV-FR37WUO",
        "outputId": "4c5f912e-37f9-41da-819b-b0393d1d0ffc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('street', 0.8598687052726746), ('mansion', 0.8576087355613708), ('streets', 0.8573037981987), ('crew', 0.8540593981742859), ('strip', 0.8438909649848938), ('kids', 0.842349648475647), ('crib', 0.8404805064201355), ('hood', 0.8374512195587158), ('square', 0.8360146284103394), ('kitchen', 0.8335614800453186)]\n"
          ]
        }
      ],
      "source": [
        "# example 3: rich->home, poor->street\n",
        "\n",
        "print(model.most_similar(positive=['rich', 'house'], negative=['poor']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGcYNghs7hpT",
        "outputId": "406430ee-ebfa-4ea0-d418-3928e4b76334"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('black', 0.866184413433075), ('tank', 0.8443092107772827), ('virgin', 0.8251262903213501), ('shoe', 0.8221466541290283), ('purple', 0.820654571056366), ('gucci', 0.8182962536811829), ('circle', 0.8180679678916931), ('hood', 0.8144944906234741), ('models', 0.8128857612609863), ('blue', 0.8073925971984863)]\n"
          ]
        }
      ],
      "source": [
        "# example 4: bruhhhhhhhhhhhhhh rich->white, poor->black\n",
        "print(model.most_similar(positive=['rich', 'white'], negative=['poor']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "902RJydFyjMx"
      },
      "source": [
        "**<font color=\"red\">[Task]</font>** Thinking About Bias.\n",
        "\n",
        "**Hint:** Briefly explain how bias can be introduced into word embeddings and suggest one method to mitigate these biases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8zWOHg3065B"
      },
      "source": [
        "**<font color=\"red\">Write your answer here.</font>**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**How bias can be introduced to word embeddings?**\n",
        "\n",
        "- Biased from the corpus used for training: If the training corpus is small or biased, words may not be placed in a way that accurately reflects their meaning. We, as human, are biased, the commonly seen social bias appear everywhere in our life, so the words we choose are biased, hence the corpus are biased.\n",
        "- If the corpus lacks enough context variety, words that rarely co-occur with others might get embeddings that are not truly representative.\n",
        "\n",
        "**How to mitigate?**\n",
        "- Consider entire sentences instead of fixed embeddings. However within the framework of Word2vec, this is not possible\n",
        "- Debiasing via Projection Removal (e.g., Hard Debiasing) – This method, introduced by Bolukbasi et al. (2016), involves identifying a \"bias direction\" in the embedding space (e.g., a gender direction) and neutralizing word vectors along this dimension."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqDmuu7m_PB5"
      },
      "source": [
        "## Supplementary Materials\n",
        "Source from [DeepNLP-Course of DanAnastasyev](https://colab.research.google.com/drive/1o65wrq6RYgWyyMvNP8r9ZknXBniDoXrn#forceEdit=true&offline=true&sandboxMode=true)\n",
        "\n",
        "## To read\n",
        "### Blogs\n",
        "[On word embeddings - Part 1, Sebastian Ruder](http://ruder.io/word-embeddings-1/)  \n",
        "[On word embeddings - Part 2: Approximating the Softmax, Sebastian Ruder](http://ruder.io/word-embeddings-softmax/index.html)  \n",
        "[Word2Vec Tutorial - The Skip-Gram Model, Chris McCormick](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)  \n",
        "[Word2Vec Tutorial Part 2 - Negative Sampling, Chris McCormick](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)\n",
        "\n",
        "### Papers\n",
        "[Word2vec Parameter Learning Explained (2014), Xin Rong](https://arxiv.org/abs/1411.2738)  \n",
        "[Neural word embedding as implicit matrix factorization (2014), Levy, Omer, and Yoav Goldberg](http://u.cs.biu.ac.il/~nlp/wp-content/uploads/Neural-Word-Embeddings-as-Implicit-Matrix-Factorization-NIPS-2014.pdf)  \n",
        "\n",
        "### Enhancing Embeddings\n",
        "[Two/Too Simple Adaptations of Word2Vec for Syntax Problems (2015), Ling, Wang, et al.](https://www.aclweb.org/anthology/N/N15/N15-1142.pdf)  \n",
        "[Not All Neural Embeddings are Born Equal (2014)](https://arxiv.org/pdf/1410.0718.pdf)  \n",
        "[Retrofitting Word Vectors to Semantic Lexicons (2014), M. Faruqui, et al.](https://arxiv.org/pdf/1411.4166.pdf)  \n",
        "[All-but-the-top: Simple and Effective Postprocessing for Word Representations (2017), Mu, et al.](https://arxiv.org/pdf/1702.01417.pdf)  \n",
        "\n",
        "### Sentence Embeddings\n",
        "[Skip-Thought Vectors (2015), Kiros, et al.](https://arxiv.org/pdf/1506.06726)  \n",
        "\n",
        "### Backpropagation\n",
        "[Backpropagation, Intuitions, cs231n + next parts in the Module 1](http://cs231n.github.io/optimization-2/)   \n",
        "[Calculus on Computational Graphs: Backpropagation, Christopher Olah](http://colah.github.io/posts/2015-08-Backprop/)\n",
        "\n",
        "## To watch\n",
        "[cs224n \"Lecture 2 - Word Vector Representations: word2vec\"](https://www.youtube.com/watch?v=ERibwqs9p38&index=2&list=PLqdrfNEc5QnuV9RwUAhoJcoQvu4Q46Lja&t=0s)  \n",
        "[cs224n \"Lecture 5 - Backpropagation\"](https://www.youtube.com/watch?v=isPiE-DBagM&index=5&list=PLqdrfNEc5QnuV9RwUAhoJcoQvu4Q46Lja&t=0s)   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ_O-O0wkool"
      },
      "source": [
        "\n",
        "## Acknowledgement\n",
        "\n",
        "This assignment was developed with reference to the following course materials:\n",
        "- [DeepNLP Course by Dan Anastasyev](https://github.com/DanAnastasyev/DeepNLP-Course?tab=readme-ov-file)\n",
        "- [Exploring Word Vectors from Stanford's CS224N](https://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring_word_vectors.html)\n",
        "- [Natural Language Processing course from Princeton University](https://nlp.cs.princeton.edu/cos484-sp21/)\n",
        "- [Yandex Data School NLP Course Week 1 Seminar](https://colab.research.google.com/github/yandexdataschool/nlp_course/blob/2023/week01_embeddings/seminar.ipynb#scrollTo=9m7GZWVk-jrW)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
